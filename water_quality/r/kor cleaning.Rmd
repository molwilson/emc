---
title: "Kor Cleaning"
author: "Molly Wilson"
date: "2026-01-16"
output: html_document
---

```{r, message = F, warning = F, echo = F}
library(tidyverse)
library(here) 
library(readxl) 
library(janitor)
library(scales)
library(lubridate)

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r}
# import raw YSI data from Kor .csv export
raw <- read_csv(here("water_quality", "data_raw", "Kor Measurement File Export.csv"),
  col_names = FALSE,
  show_col_types = FALSE) %>%
  filter(!str_detect(X1, "MEAN VALUE|STANDARD DEVIATION|SENSOR SERIAL NUMBER"))

# import list of entries to be omitted
omissions <- read_excel(here("water_quality", "data_raw", "Water quality data.xlsx"), sheet = "ysi omissions") %>%
  mutate(time = format(time, "%H:%M:%S"),
         date = as.Date(date))
### need to reformat date/times

# import list of entries to be corrected
corrections <- read_excel(here("water_quality", "data_raw", "Water quality data.xlsx"), sheet = "ysi corrections") %>%
  mutate(time = format(time, "%H:%M:%S"),
         date = as.Date(date))
```

Cleaning YSI/Kor .csv
```{r}
# identify header rows for each sensor type
header_rows_turbidity <- which(apply(raw, 1, function(row)
  any(str_detect(as.character(row), "TURBIDITY FNU"))
))

header_rows_gen <- which(apply(raw, 1, function(row)
  any(str_detect(as.character(row), "TAL PE RFU"))
))

# identify data rows for each sensor type
data_rows_turbidity <- header_rows_turbidity + 1
data_rows_gen       <- header_rows_gen + 1

# function for cleaning data into one tibble per sensor
clean_sensor <- function(header_rows, data_rows, raw_df) {

  blocks <- map2(header_rows, data_rows, ~{

    header <- as.character(unlist(raw_df[.x, ]))
    header <- iconv(header, from = "", to = "UTF-8", sub = "")

    empty_idx <- which(is.na(header) | header == "")
    header[empty_idx] <- paste0("empty_col_", seq_along(empty_idx))

    header <- make.names(header, unique = TRUE)
    header <- make_clean_names(header)

    data <- as.character(unlist(raw_df[.y, ]))

    df <- as_tibble(as.list(data), .name_repair = "minimal")
    colnames(df) <- header

    df
  })

  tidy_df <- bind_rows(blocks) %>%
    mutate(across(everything(), ~ type.convert(., as.is = TRUE))) %>%
    mutate(date = as.Date(date_m_d_yyyy, format = "%m/%d/%y"),
           time = format(
             lubridate::parse_date_time(time_h_mm_ss_tt, orders = "I:M:S p"),
             "%H:%M:%S"))

  tidy_df
}

# creating clean dfs for turbidity and general sensor data
turbidity <- clean_sensor(header_rows_turbidity, data_rows_turbidity, raw) %>%
  select(date, time, site_code_depth = site_name, turbidity_fnu) %>%
  anti_join(omissions, by = c("date", "time", "site_code_depth")) %>%
  left_join(corrections, by = c("date", "time", "site_code_depth")) %>%
  mutate(site_code_depth = coalesce(site_corrected, site_code_depth)) %>%
  select(-site_corrected)

general <- clean_sensor(header_rows_gen, data_rows_gen, raw) %>%
  select(date, time, site_code_depth = site_name, tal_pe_rfu, chlorophyll_rfu, cond_s_cm, spcond_s_cm, tds_mg_l, sal_psu, odo_mg_l, ph, temp_c) %>%
  anti_join(omissions, by = c("date", "time", "site_code_depth")) %>%
  left_join(corrections, by = c("date", "time", "site_code_depth")) %>%
  mutate(site_code_depth = coalesce(site_corrected, site_code_depth)) %>%
  select(-site_corrected)

# checking that corrections were applied
chk_turbidity <- turbidity %>%
  inner_join(corrections %>% select(date, time), 
             by = c("date", "time"))
chk_general <- general %>%
  inner_join(corrections %>% select(date, time), 
             by = c("date", "time"))

# creating one df with both sensor groups
ysi <- general %>%
  select(-time) %>%
  full_join(turbidity %>%
              select(-time),
            by = c("date", "site_code_depth")) %>%
  mutate(site_code_depth = case_when(site_code_depth == "Corington" ~ "COV",
                                     site_code_depth == "greenhEB" ~ "GEB",
                                     site_code_depth == "SH1" ~ "SRS1",
                                     TRUE ~ toupper(site_code_depth)),
         site_code = str_extract_all(site_code_depth, "[A-Za-z]") %>% 
           sapply(paste0, collapse = ""),
         depth = str_extract_all(site_code_depth, "\\d") %>% 
           sapply(paste0, collapse = "") %>% 
           as.integer(),
         depth = coalesce(depth, 1)) %>% # replacing sites with no depth as depth = 1
  relocate(date, site_code_depth, site_code, depth) %>%
  rename(do_mg_l = odo_mg_l) %>%
  set_names(~ str_replace_all(., "_s_", "_us_")) # replacing "u" symbol lost when stripping data

# checking for duplicated/incomplete data
chk_ysi1 <- ysi %>% 
  group_by(site_code_depth, date) %>%
  summarize(count = n()) %>%
  filter(count != 1)

chk_ysi2 <- ysi %>%
  filter(is.na(tal_pe_rfu) | is.na(turbidity_fnu))

# export csv and clean env.
write_csv(ysi, here("water_quality", "data_outputs", "ysi_clean.csv"))
rm(list = ls(pattern = "^chk"))
```



